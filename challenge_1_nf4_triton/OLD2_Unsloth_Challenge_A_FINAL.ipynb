{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "So2Ou1rEilxI"
      },
      "outputs": [],
      "source": [
        "# Code to install Unsloth, Triton, Torch etc\n",
        "%%capture\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################################\n",
        "# 1) Existing puzzle snippet: MLP, test harness, etc.\n",
        "##############################################################################\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import unsloth.kernels.utils as uutils\n",
        "from transformers import set_seed\n",
        "import time\n",
        "import inspect\n",
        "import os\n",
        "\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "HAS_BFLOAT16 = (major_version >= 8)\n",
        "\n",
        "from inspect import currentframe as _C, getframeinfo\n",
        "_F = lambda c: getframeinfo(c).lineno\n",
        "WARN = lambda x: print(f\"\\033[31m{x}\\033[0m\")\n",
        "\n",
        "def NAME(var):\n",
        "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
        "    names = [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
        "    return names[0] if len(names) != 0 else \"\"\n",
        "\n",
        "def assert_same(x, y, line, dtype):\n",
        "    assert(x.dtype == dtype)\n",
        "    try:\n",
        "        torch.testing.assert_close(x, y, check_stride=True)\n",
        "    except Exception as error:\n",
        "        raise RuntimeError(\n",
        "            f\"Failed allclose at line [{line}]: {NAME(x)}, {NAME(y)}\\n{str(error)}\"\n",
        "        )\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "\n",
        "from bitsandbytes.nn import Linear4bit\n",
        "from transformers.activations import ACT2FN\n",
        "from unsloth.kernels.utils import fast_dequantize\n",
        "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
        "\n",
        "def unsloth_dequantize(weight):\n",
        "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
        "\n",
        "def bnb_Linear4bit(hd, m, dtype=torch.float16):\n",
        "    return Linear4bit(\n",
        "        hd, m, bias=None,\n",
        "        compute_dtype=dtype,\n",
        "        compress_statistics=True,\n",
        "        quant_type=\"nf4\",\n",
        "    )\n",
        "\n",
        "def assert_correct_bnb(weight, dtype):\n",
        "    assert(weight.weight.dtype == torch.uint8)\n",
        "    qs = weight.weight.quant_state\n",
        "    assert(qs.dtype == dtype)\n",
        "    assert(qs.absmax.dtype == torch.uint8)\n",
        "    assert(qs.code.dtype == torch.float32)\n",
        "    assert(qs.offset.dtype == torch.float32)\n",
        "    assert(qs.blocksize == 64)\n",
        "    st2 = qs.state2\n",
        "    assert(st2.absmax.dtype == torch.float32)\n",
        "    assert(st2.code.dtype == torch.float32)\n",
        "    assert(st2.blocksize == 256)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hd=4096, m=14336, dtype=torch.float16):\n",
        "        super().__init__()\n",
        "        self.gate_proj = bnb_Linear4bit(hd, m, dtype=dtype).to(\"cuda\")\n",
        "        self.up_proj   = bnb_Linear4bit(hd, m, dtype=dtype).to(\"cuda\")\n",
        "        self.down_proj = bnb_Linear4bit(m, hd, dtype=dtype).to(\"cuda\")\n",
        "        self.gate_proj.weight.quant_state.dtype = dtype\n",
        "        self.up_proj  .weight.quant_state.dtype = dtype\n",
        "        self.down_proj.weight.quant_state.dtype = dtype\n",
        "        self.act_fn = ACT2FN[\"silu\"]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "\n",
        "def mlp_forward(X, mlp, fx):\n",
        "    up   = X @ fx(mlp.up_proj).t()\n",
        "    gate = X @ fx(mlp.gate_proj).t()\n",
        "    h = mlp.act_fn(gate) * up\n",
        "    down = h @ fx(mlp.down_proj).t()\n",
        "    return down\n",
        "\n",
        "def mlp_dequantize(X, mlp, fx):\n",
        "    a = fx(mlp.up_proj).t(); torch.cuda.synchronize()\n",
        "    b = fx(mlp.gate_proj).t(); torch.cuda.synchronize()\n",
        "    c = fx(mlp.down_proj).t(); torch.cuda.synchronize()\n",
        "    return a, b, c\n",
        "\n",
        "def test_dequantize(dequantize_fx):\n",
        "    elapsed = 0\n",
        "    options = [\n",
        "        (2, 3333, 2048, 8192, 3407, torch.float16),\n",
        "        (5,  777, 1024, 4096, 3409, torch.bfloat16),\n",
        "        (3, 2048, 4096, 14336, 3408, torch.bfloat16),\n",
        "    ]\n",
        "    for (bsz, qlen, hd, m, seed, dt) in options:\n",
        "        set_seed(seed)\n",
        "        torch.set_default_dtype(torch.float32)\n",
        "        mlp = MLP(hd=hd, m=m, dtype=dt)\n",
        "        X = torch.randn((bsz, qlen, hd), device=\"cuda\", dtype=dt)\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        # Warmup\n",
        "        for _ in range(2):\n",
        "            assert_same(mlp_forward(X, mlp, dequantize_fx), mlp(X), _F(_C()), dt)\n",
        "            assert_correct_bnb(mlp.up_proj, dt)\n",
        "            assert_correct_bnb(mlp.gate_proj, dt)\n",
        "            assert_correct_bnb(mlp.down_proj, dt)\n",
        "            a, b, c = mlp_dequantize(X, mlp, dequantize_fx)\n",
        "            A, B, C = mlp_dequantize(X, mlp, unsloth_dequantize)\n",
        "            assert_same(a, A, _F(_C()), dt)\n",
        "            assert_same(b, B, _F(_C()), dt)\n",
        "            assert_same(c, C, _F(_C()), dt)\n",
        "\n",
        "        # Benchmark\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "        for _ in range(1000):\n",
        "            mlp_dequantize(X, mlp, dequantize_fx)\n",
        "        elapsed += time.time() - start\n",
        "    return elapsed\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "# 2) Final \"warp-streaming parallel dequant\" approach:\n",
        "#    - warp-level pipe: uses tl.async_commit_group()\n",
        "#    - uses tl.tensor_dot() for nib-lut decode\n",
        "#    - tries to push past memory scheduling issues\n",
        "##############################################################################\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "def final_warp_config(shape):\n",
        "    \"\"\"\n",
        "    We'll pick some synergy for warp-based streaming.\n",
        "    Possibly smaller warps or moderate blocks.\n",
        "    This can be tuned.\n",
        "    \"\"\"\n",
        "    bsz, qlen = shape\n",
        "    # We'll fix e.g. warps=8, stages=4 as a guess for streaming pipeline\n",
        "    num_warps = 128\n",
        "    num_stages = 2\n",
        "    return num_warps, num_stages\n",
        "\n",
        "@triton.jit\n",
        "def _nf4_dequant_warp_streaming_kernel(\n",
        "    W_PTR, CODE_PTR, ABS_PTR, ABS2_PTR, OFFSET,\n",
        "    Out_PTR,\n",
        "    total_elems,\n",
        "    blocksize,\n",
        "    blocksize2,\n",
        "    out_dtype_flag,\n",
        "    BLOCKS_PER_WARP: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    Single kernel with \"warp-streaming\" approach:\n",
        "    - uses tl.async_commit_group() to pipeline\n",
        "    - uses tl.tensor_dot() for nib decode\n",
        "    - tries to override Triton's default memory scheduling\n",
        "    \"\"\"\n",
        "    pid = tl.program_id(0)\n",
        "    warp_start = pid * BLOCKS_PER_WARP\n",
        "\n",
        "    n_blocks = total_elems // blocksize\n",
        "\n",
        "    # load code LUT => shape [16]\n",
        "    code_lut = tl.load(CODE_PTR + tl.arange(0, 16))\n",
        "\n",
        "    for i in range(BLOCKS_PER_WARP):\n",
        "        blk_id = warp_start + i\n",
        "        if blk_id >= n_blocks:\n",
        "            break\n",
        "\n",
        "        nib_start = blk_id * blocksize\n",
        "        if nib_start >= total_elems:\n",
        "            break\n",
        "\n",
        "        # compute final scale => (abs/127)*abs2 + offset\n",
        "        abs_u8 = tl.load(ABS_PTR + blk_id)\n",
        "        abs_f32 = tl.cast(abs_u8, tl.float32) / 127.0\n",
        "        abs2_f32 = tl.load(ABS2_PTR + blk_id)\n",
        "        final_scale = abs_f32 * abs2_f32 + OFFSET\n",
        "\n",
        "        # read nib-coded => 32 bytes\n",
        "        byte_start = nib_start // 2\n",
        "        base_addr = byte_start + tl.arange(0, 32)\n",
        "        mask = base_addr < ((total_elems + 1)//2)\n",
        "        chunk = tl.load(W_PTR + base_addr, mask=mask, other=0)\n",
        "\n",
        "        # pipeline => commit\n",
        "        tl.async_commit_group()\n",
        "\n",
        "        # decode nib => warp-level => use tl.tensor_dot\n",
        "        nib_lo = chunk & 0xF\n",
        "        nib_hi = chunk >> 4\n",
        "\n",
        "        # shape => [32], we do one-hot => then tl.tensor_dot => partial\n",
        "        # or direct gather => if newer triton allows gather. We'll do partial approach:\n",
        "        # We'll do something akin to a vector approach:\n",
        "        # For demonstration, do partial one-hot =>\n",
        "        # If older Triton balks, you'll need to adapt.\n",
        "        nidx = tl.arange(0, 16)\n",
        "        nib_lo_broad = nib_lo[:, None]\n",
        "        onehot_lo = tl.where(nib_lo_broad == nidx[None, :], 1.0, 0.0)\n",
        "        decoded_lo = tl.tensor_dot(onehot_lo, code_lut)* final_scale\n",
        "\n",
        "        nib_hi_broad = nib_hi[:, None]\n",
        "        onehot_hi = tl.where(nib_hi_broad == nidx[None, :], 1.0, 0.0)\n",
        "        decoded_hi = tl.tensor_dot(onehot_hi, code_lut)* final_scale\n",
        "\n",
        "        # store => 64\n",
        "        for j in range(32):\n",
        "            outLo = nib_start + 2*j\n",
        "            outHi = outLo + 1\n",
        "            if outLo < total_elems:\n",
        "                val_lo = decoded_lo[j]\n",
        "                if out_dtype_flag==0:\n",
        "                    val_lo = tl.cast(val_lo, tl.float16)\n",
        "                else:\n",
        "                    val_lo = tl.cast(val_lo, tl.bfloat16)\n",
        "                tl.store(Out_PTR + outLo, val_lo)\n",
        "            if outHi < total_elems:\n",
        "                val_hi = decoded_hi[j]\n",
        "                if out_dtype_flag==0:\n",
        "                    val_hi = tl.cast(val_hi, tl.float16)\n",
        "                else:\n",
        "                    val_hi = tl.cast(val_hi, tl.bfloat16)\n",
        "                tl.store(Out_PTR + outHi, val_hi)\n",
        "\n",
        "def final_warp_streaming_dequant(weight, quant_state=None, out=None):\n",
        "    \"\"\"\n",
        "    Single kernel approach that tries to do warp-level streaming\n",
        "    with tl.async_commit_group, tl.tensor_dot,\n",
        "    in hopes of surpassing 1.15x speed\n",
        "    \"\"\"\n",
        "    if quant_state is None:\n",
        "        return weight\n",
        "\n",
        "    absmax  = quant_state.absmax\n",
        "    shape   = quant_state.shape\n",
        "    dt      = quant_state.dtype\n",
        "    bsz     = quant_state.blocksize\n",
        "    offset  = quant_state.offset\n",
        "    st2     = quant_state.state2\n",
        "    absmax2 = st2.absmax\n",
        "    code2   = st2.code\n",
        "    bsz2    = st2.blocksize\n",
        "\n",
        "    offset_val = float(offset.item()) if isinstance(offset, torch.Tensor) else float(offset)\n",
        "    n_elems = shape[0]*shape[1]\n",
        "    if (n_elems%bsz)!=0:\n",
        "        raise ValueError(\"Multiple of 64 required\")\n",
        "\n",
        "    if out is None:\n",
        "        out = torch.empty(shape, dtype=dt, device=weight.device, requires_grad=False)\n",
        "    else:\n",
        "        assert out.shape == shape and out.dtype == dt\n",
        "\n",
        "    is_transposed = (weight.shape[0]==1)\n",
        "    out_dtype_flag = 0 if dt==torch.float16 else 1\n",
        "\n",
        "    n_blocks = n_elems//bsz\n",
        "    BLOCKS_PER_WARP=256\n",
        "    # pick warps/stages from final_warp_config\n",
        "    bsz_, qlen_ = shape\n",
        "    num_warps, num_stages = final_warp_config((bsz_, qlen_))\n",
        "\n",
        "    import math\n",
        "    grid_dim = math.ceil(n_blocks / BLOCKS_PER_WARP)\n",
        "\n",
        "    _nf4_dequant_warp_streaming_kernel[grid_dim](\n",
        "        weight,\n",
        "        code2,\n",
        "        absmax,\n",
        "        absmax2,\n",
        "        offset_val,\n",
        "        out,\n",
        "        n_elems,\n",
        "        bsz,\n",
        "        bsz2,\n",
        "        out_dtype_flag,\n",
        "        BLOCKS_PER_WARP=BLOCKS_PER_WARP,\n",
        "        num_warps=num_warps,\n",
        "        num_stages=num_stages\n",
        "    )\n",
        "\n",
        "    if is_transposed:\n",
        "        return out.t()\n",
        "    return out\n",
        "\n",
        "\n",
        "def test_dequantize_final_warp_streaming():\n",
        "    \"\"\"\n",
        "    Overwrite puzzle decode => measure speed\n",
        "    Expect >=1.15x if warp-streaming approach helps\n",
        "    \"\"\"\n",
        "    import unsloth.kernels.utils as uutils\n",
        "    if \"_original_fast_dequantize\" not in globals():\n",
        "        globals()[\"_original_fast_dequantize\"] = uutils.fast_dequantize\n",
        "\n",
        "    # override decode\n",
        "    uutils.fast_dequantize = final_warp_streaming_dequant\n",
        "\n",
        "    print(\"\\n--- Testing 'Warp-Streaming Parallel Dequant' Single-Kernel Approach ---\")\n",
        "    time_new = test_dequantize(unsloth_dequantize)\n",
        "    print(f\"[Warp-Streaming Approach] => {time_new:.4f}s\")\n",
        "\n",
        "    # restore\n",
        "    uutils.fast_dequantize = globals()[\"_original_fast_dequantize\"]\n",
        "    print(\"\\n--- Testing unsloth original approach ---\")\n",
        "    time_old = test_dequantize(unsloth_dequantize)\n",
        "    print(f\"[Unsloth Original] => {time_old:.4f}s\")\n",
        "\n",
        "    speedup = time_old / time_new\n",
        "    print(f\"Speedup => {speedup:.2f}x\")\n"
      ],
      "metadata": {
        "id": "HMLMHGvOion5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e76ddc69-857f-4f1d-d682-2e7ca6ad71f0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dequantize_final_warp_streaming()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R45_QVNpSwvH",
        "outputId": "e55cce2b-b77a-4d1b-b4ad-e4523d69a1d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing 'Warp-Streaming Parallel Dequant' Single-Kernel Approach ---\n",
            "[Warp-Streaming Approach] => 5.6515s\n",
            "\n",
            "--- Testing unsloth original approach ---\n",
            "[Unsloth Original] => 7.1430s\n",
            "Speedup => 1.26x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dequantize_final_warp_streaming()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i2H-PmMvUXv",
        "outputId": "1619f790-ff7a-482d-9187-c420c22fefb3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing 'Warp-Streaming Parallel Dequant' Single-Kernel Approach ---\n",
            "[Warp-Streaming Approach] => 5.5614s\n",
            "\n",
            "--- Testing unsloth original approach ---\n",
            "[Unsloth Original] => 6.1355s\n",
            "Speedup => 1.10x\n"
          ]
        }
      ]
    }
  ]
}
