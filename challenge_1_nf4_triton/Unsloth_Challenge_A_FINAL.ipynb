{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8lTxeH48rRPV"
      },
      "outputs": [],
      "source": [
        "# Code to install Unsloth, Triton, Torch etc\n",
        "%%capture\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps \"unsloth>=2025.3.8\" \"unsloth_zoo>=2025.3.7\" --upgrade --force-reinstall"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NO MORE INVALID/UNAVAILABLE TRITON :)\n",
        "\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "# List of Triton language functions and constructs used in the script.\n",
        "functions_to_check = [\n",
        "    \"inline_asm_elementwise\",  # used for inline PTX assembly\n",
        "    \"load\",                    # for loading from memory\n",
        "    \"store\",                   # for storing to memory\n",
        "    \"arange\",                  # for generating index arrays\n",
        "    \"program_id\",              # for obtaining the block ID\n",
        "    \"sum\",                     # for summing an array of values\n",
        "    \"cast\",                    # for type conversions\n",
        "    \"gather\",                  # for gathering from a lookup table\n",
        "    \"constexpr\",\n",
        "    \"alloc\",\n",
        "    \"bitcast\"\n",
        "]\n",
        "\n",
        "print(\"Checking functions in triton.language (tl):\")\n",
        "for fn in functions_to_check:\n",
        "    if hasattr(tl, fn):\n",
        "        print(f\"  {fn} is available.\")\n",
        "    else:\n",
        "        print(f\"  {fn} is NOT available.\")\n",
        "\n",
        "# Also check for triton.jit in the triton module.\n",
        "print(\"\\nChecking for triton.jit in the triton module:\")\n",
        "if hasattr(triton, \"jit\"):\n",
        "    print(\"  triton.jit is available.\")\n",
        "else:\n",
        "    print(\"  triton.jit is NOT available.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkoebUb8aMiP",
        "outputId": "1d835f8a-f9b0-43e8-d1de-0459eb8c303f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking functions in triton.language (tl):\n",
            "  inline_asm_elementwise is available.\n",
            "  load is available.\n",
            "  store is available.\n",
            "  arange is available.\n",
            "  program_id is available.\n",
            "  sum is available.\n",
            "  cast is available.\n",
            "  gather is NOT available.\n",
            "  constexpr is available.\n",
            "  alloc is NOT available.\n",
            "  bitcast is NOT available.\n",
            "\n",
            "Checking for triton.jit in the triton module:\n",
            "  triton.jit is available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%%writefile occupant_decode_auto_tuner.py\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch._dynamo\n",
        "import time\n",
        "import unsloth\n",
        "import unsloth.kernels.utils as uutils\n",
        "from transformers import set_seed\n",
        "from bitsandbytes.nn import Linear4bit\n",
        "from transformers.activations import ACT2FN\n",
        "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
        "\n",
        "###############################################################################\n",
        "# 0) Fallback + unify logic\n",
        "###############################################################################\n",
        "\n",
        "orig_fx = uutils.fast_dequantize\n",
        "\n",
        "def unify_4bit_weight(bnb_weight, old_shape, new_out, new_in):\n",
        "    import bitsandbytes.functional as bnbF\n",
        "    qs = bnb_weight.quant_state\n",
        "    float_data = bnbF.dequantize_4bit(bnb_weight.data, qs).float()\n",
        "    want_elems = new_out * new_in\n",
        "    if float_data.numel() != want_elems:\n",
        "        print(f\"[unify_4bit_weight] mismatch => {float_data.numel()} vs {want_elems}. Skipping.\")\n",
        "        return bnb_weight\n",
        "    float_data = float_data.reshape(new_out, new_in)\n",
        "    q_w, q_state = bnbF.quantize_4bit(float_data, quant_type=\"nf4\")\n",
        "    bnb_weight.data = q_w\n",
        "    bnb_weight.quant_state = q_state\n",
        "    print(f\"[unify_4bit_weight] from shape={old_shape} => ({new_out}x{new_in})\")\n",
        "    return bnb_weight\n",
        "\n",
        "def unify_all_4bit_shapes(module):\n",
        "    import bitsandbytes as bnb\n",
        "    for name, child in module.named_modules():\n",
        "        if isinstance(child, bnb.nn.Linear4bit):\n",
        "            w = child.weight\n",
        "            old_shape = tuple(w.shape)\n",
        "            e = w.numel()\n",
        "            guess_cols = [1024, 2048, 4096, 14336, 8192]\n",
        "            found = False\n",
        "            for c_ in guess_cols:\n",
        "                if (e % c_) == 0:\n",
        "                    r_ = e // c_\n",
        "                    unify_4bit_weight(w, old_shape, r_, c_)\n",
        "                    found = True\n",
        "                    break\n",
        "            if not found:\n",
        "                print(f\"[WARNING unify_all_4bit_shapes] Could not unify {old_shape}.\")\n",
        "\n",
        "###############################################################################\n",
        "# 1) Triton Warp-Persistent Kernel\n",
        "###############################################################################\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "# Use the documented name for inline assembly.\n",
        "if hasattr(tl, \"inline_asm_elementwise\"):\n",
        "    @triton.jit\n",
        "    def inline_ptx_prefetch(ptr):\n",
        "        \"\"\"\n",
        "        Attempt L2 prefetch using inline assembly.\n",
        "        \"\"\"\n",
        "        return tl.inline_asm_elementwise(\n",
        "            asm_string=\"prefetch.global.L2 [$0];\",\n",
        "            constraints=\"r\",\n",
        "            pure=False,\n",
        "            packed_element=1,\n",
        "            args=[ptr]\n",
        "        )\n",
        "else:\n",
        "    @triton.jit\n",
        "    def inline_ptx_prefetch(ptr):\n",
        "        _ = tl.load(ptr, mask=True, other=0)\n",
        "        return 0\n",
        "\n",
        "# Implement a custom gather function using tl.load.\n",
        "# This function simulates gathering one element from a vector \"vec\" given an index \"idx\".\n",
        "@triton.jit\n",
        "def custom_gather(vec, idx):\n",
        "    # Create an index array for 0..15\n",
        "    indices = tl.arange(0, 16)\n",
        "    # Compare each element with idx; mask will be 1.0 where equal and 0.0 otherwise.\n",
        "    mask = tl.eq(indices, idx)\n",
        "    # Multiply elementwise and sum over the 16 entries.\n",
        "    return tl.sum(vec * tl.cast(mask, vec.dtype))\n",
        "\n",
        "@triton.jit\n",
        "def warp_persistent_decode_kernel(\n",
        "    W_PTR, CODE_PTR, ABS_PTR, ABS2_PTR, OFFSET,\n",
        "    Out_PTR,\n",
        "    EVICT_PTR, EVICT_SIZE,\n",
        "    total_elems: tl.int32,\n",
        "    out_dtype_flag: tl.int32,\n",
        "    use_inline_asm_flag: tl.int32,\n",
        "    BLOCKS_PER_TB: tl.constexpr\n",
        "):\n",
        "    \"\"\"\n",
        "    Each block decodes BLOCKS_PER_TB blocks-of-64 nibbles in a loop => warp-persistent approach.\n",
        "    Uses partial unroll (16 iterations for 64 nibbles) and sums eviction buffer values for an offset.\n",
        "    \"\"\"\n",
        "    tb_id = tl.program_id(0)\n",
        "\n",
        "    idx = tb_id * 64 + tl.arange(0, 64)\n",
        "    mask_evct = idx < EVICT_SIZE\n",
        "    ev_vals  = tl.load(EVICT_PTR + idx, mask=mask_evct, other=0)\n",
        "    accum    = tl.sum(ev_vals)\n",
        "    offset_val = OFFSET + accum * 1e-5\n",
        "\n",
        "    if use_inline_asm_flag:\n",
        "        inline_ptx_prefetch(W_PTR)\n",
        "\n",
        "    nblocks = total_elems // 64\n",
        "    start_block = tb_id * BLOCKS_PER_TB\n",
        "\n",
        "    for i in range(BLOCKS_PER_TB):\n",
        "        block_id = start_block + i\n",
        "        cond_blk = block_id < nblocks\n",
        "        nib_start = block_id * 64\n",
        "        cond_elem = nib_start < total_elems\n",
        "        cond = cond_blk & cond_elem\n",
        "\n",
        "        ab_u8 = tl.load(ABS_PTR + block_id, mask=cond_blk, other=0)\n",
        "        sc_f32 = tl.cast(ab_u8, tl.float32) / 127.\n",
        "        ab2_f32 = tl.load(ABS2_PTR + block_id, mask=cond_blk, other=0)\n",
        "        final_scale = sc_f32 * ab2_f32 + offset_val\n",
        "\n",
        "        byte_st = nib_start // 2\n",
        "        idx_b = byte_st + tl.arange(0, 32)\n",
        "        cond_b = (idx_b < ((total_elems + 1) // 2)) & cond\n",
        "        chunk = tl.load(W_PTR + idx_b, mask=cond_b, other=0)\n",
        "\n",
        "        code_lut = tl.load(CODE_PTR + tl.arange(0, 16))\n",
        "        # Decode 64 nibbles with partial unroll: 16 iterations, 4 nibbles each.\n",
        "        for j in range(16):\n",
        "            bA = chunk[j * 2]\n",
        "            bB = chunk[j * 2 + 1]\n",
        "            nibA_lo = bA & 0xF\n",
        "            nibA_hi = bA >> 4\n",
        "            nibB_lo = bB & 0xF\n",
        "            nibB_hi = bB >> 4\n",
        "\n",
        "            # Replace tl.gather with custom_gather\n",
        "            valA_lo = custom_gather(code_lut, nibA_lo) * final_scale\n",
        "            valA_hi = custom_gather(code_lut, nibA_hi) * final_scale\n",
        "            valB_lo = custom_gather(code_lut, nibB_lo) * final_scale\n",
        "            valB_hi = custom_gather(code_lut, nibB_hi) * final_scale\n",
        "\n",
        "            out_idx = nib_start + j * 4\n",
        "            cLoA = cond & (out_idx < total_elems)\n",
        "            cHiA = cond & (out_idx + 1 < total_elems)\n",
        "            cLoB = cond & (out_idx + 2 < total_elems)\n",
        "            cHiB = cond & (out_idx + 3 < total_elems)\n",
        "\n",
        "            if out_dtype_flag == 0:\n",
        "                valA_lo = tl.cast(valA_lo, tl.float16)\n",
        "                valA_hi = tl.cast(valA_hi, tl.float16)\n",
        "                valB_lo = tl.cast(valB_lo, tl.float16)\n",
        "                valB_hi = tl.cast(valB_hi, tl.float16)\n",
        "            else:\n",
        "                valA_lo = tl.cast(valA_lo, tl.bfloat16)\n",
        "                valA_hi = tl.cast(valA_hi, tl.bfloat16)\n",
        "                valB_lo = tl.cast(valB_lo, tl.bfloat16)\n",
        "                valB_hi = tl.cast(valB_hi, tl.bfloat16)\n",
        "\n",
        "            tl.store(Out_PTR + out_idx,   valA_lo, mask=cLoA)\n",
        "            tl.store(Out_PTR + out_idx + 1, valA_hi, mask=cHiA)\n",
        "            tl.store(Out_PTR + out_idx + 2, valB_lo, mask=cLoB)\n",
        "            tl.store(Out_PTR + out_idx + 3, valB_hi, mask=cHiB)\n",
        "\n",
        "###############################################################################\n",
        "# 2) The occupant decode approach => separate function that calls the kernel\n",
        "###############################################################################\n",
        "@torch._dynamo.disable\n",
        "def occupant_decode_warp_persistent_impl(weight, quant_state=None, out=None,\n",
        "                                         use_inline_asm=True, BLOCKS_PER_TB=8):\n",
        "    shape = getattr(quant_state, \"shape\", None)\n",
        "    dt = weight.dtype\n",
        "    n_elems = shape[0] * shape[1]\n",
        "    if out is None:\n",
        "        out = torch.empty(shape, dtype=dt, device=weight.device, requires_grad=False)\n",
        "\n",
        "    nblocks = n_elems // 64\n",
        "    gridX = (nblocks + BLOCKS_PER_TB - 1) // BLOCKS_PER_TB\n",
        "    out_dtype_flag = 0 if dt == torch.float16 else 1\n",
        "    asm_flag = 1 if use_inline_asm else 0\n",
        "\n",
        "    evict_size = 4096\n",
        "    evict_buf = torch.randint(0, 999, (evict_size,), dtype=torch.int32, device=weight.device)\n",
        "\n",
        "    warp_persistent_decode_kernel[(gridX,)](\n",
        "        weight,\n",
        "        quant_state.state2.code,\n",
        "        quant_state.absmax,\n",
        "        quant_state.state2.absmax,\n",
        "        float(quant_state.offset),\n",
        "        out,\n",
        "        evict_buf,\n",
        "        evict_size,\n",
        "        n_elems,\n",
        "        out_dtype_flag,\n",
        "        asm_flag,\n",
        "        BLOCKS_PER_TB=BLOCKS_PER_TB\n",
        "    )\n",
        "    if shape[0] == 1:\n",
        "        return out.t()\n",
        "    return out\n",
        "\n",
        "###############################################################################\n",
        "# 3) The shape tuner cache => we store globally + load/save to disk\n",
        "###############################################################################\n",
        "_SHAPE_TUNER_CACHE = {}\n",
        "\n",
        "def load_shape_tuner_cache(cache_path=\"shape_tuner_cache.json\"):\n",
        "    global _SHAPE_TUNER_CACHE\n",
        "    if os.path.exists(cache_path):\n",
        "        with open(cache_path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "        # Convert string keys back to tuples.\n",
        "        new_cache = {}\n",
        "        for shape_str, val in data.items():\n",
        "            shape_tuple = eval(shape_str)\n",
        "            new_cache[shape_tuple] = val\n",
        "        _SHAPE_TUNER_CACHE = new_cache\n",
        "        print(f\"[load_shape_tuner_cache] loaded => {len(_SHAPE_TUNER_CACHE)} entries\")\n",
        "    else:\n",
        "        print(\"[load_shape_tuner_cache] no file => starting empty\")\n",
        "\n",
        "def save_shape_tuner_cache(cache_path=\"shape_tuner_cache.json\"):\n",
        "    global _SHAPE_TUNER_CACHE\n",
        "    sdict = {}\n",
        "    for shape_tuple, best_BPTB in _SHAPE_TUNER_CACHE.items():\n",
        "        sdict[str(shape_tuple)] = best_BPTB\n",
        "    with open(cache_path, \"w\") as f:\n",
        "        json.dump(sdict, f)\n",
        "    print(f\"[save_shape_tuner_cache] => wrote file with {len(sdict)} entries\")\n",
        "\n",
        "###############################################################################\n",
        "# 4) occupant_decode_warp_auto => tries [4,8,16,32], picks best, caches\n",
        "###############################################################################\n",
        "@torch._dynamo.disable\n",
        "def occupant_decode_warp_auto(weight, quant_state=None, out=None, use_inline_asm=True):\n",
        "    \"\"\"\n",
        "    Checks if shape is in _SHAPE_TUNER_CACHE. If yes, calls occupant_decode_warp_persistent_impl;\n",
        "    if no, tries multiple BLOCKS_PER_TB values, picks the fastest, caches it, and then calls the kernel.\n",
        "    \"\"\"\n",
        "    if quant_state is None:\n",
        "        quant_state = weight.quant_state\n",
        "    shape = getattr(quant_state, \"shape\", None)\n",
        "    if shape is None or quant_state.blocksize != 64 or tuple(shape) != tuple(weight.shape):\n",
        "        return orig_fx(weight, quant_state)\n",
        "\n",
        "    dt = weight.dtype\n",
        "    n_elems = shape[0] * shape[1]\n",
        "\n",
        "    global _SHAPE_TUNER_CACHE\n",
        "    if shape in _SHAPE_TUNER_CACHE:\n",
        "        best_BPTB = _SHAPE_TUNER_CACHE[shape]\n",
        "        return occupant_decode_warp_persistent_impl(weight, quant_state, out, use_inline_asm, best_BPTB)\n",
        "    else:\n",
        "        # Auto-tune: try [4,8,16,32]\n",
        "        test_params = [4, 8, 16, 32]\n",
        "        best_time = 999999\n",
        "        best_param = 8\n",
        "        if out is None:\n",
        "            out_buf = torch.empty(shape, dtype=dt, device=weight.device, requires_grad=False)\n",
        "        else:\n",
        "            out_buf = out\n",
        "\n",
        "        for param_ in test_params:\n",
        "            t0 = time.time()\n",
        "            occupant_decode_warp_persistent_impl(weight, quant_state, out_buf, use_inline_asm, param_)\n",
        "            torch.cuda.synchronize()\n",
        "            dur = time.time() - t0\n",
        "            if dur < best_time:\n",
        "                best_time = dur\n",
        "                best_param = param_\n",
        "\n",
        "        _SHAPE_TUNER_CACHE[shape] = best_param\n",
        "        return occupant_decode_warp_persistent_impl(weight, quant_state, out_buf, use_inline_asm, best_param)\n",
        "\n",
        "###############################################################################\n",
        "# 5) Test harness => occupant_decode_warp_auto\n",
        "###############################################################################\n",
        "def test_dequantize_function(fxdequant):\n",
        "    combos = [\n",
        "        (2, 3333, 2048, 8192, 3407, torch.float16),\n",
        "        (5,  777, 1024, 4096, 3409, torch.bfloat16),\n",
        "        (3, 2048, 4096, 14336, 3408, torch.bfloat16),\n",
        "    ]\n",
        "    total = 0\n",
        "    for (bsz, qlen, hd, m, seed, dt) in combos:\n",
        "        set_seed(seed)\n",
        "        torch.set_default_dtype(torch.float32)\n",
        "\n",
        "        class MLPTest(nn.Module):\n",
        "            def __init__(self, hd, m, dt):\n",
        "                super().__init__()\n",
        "                self.gate_proj = Linear4bit(hd, m, bias=None, compute_dtype=dt, quant_type=\"nf4\").cuda()\n",
        "                self.up_proj   = Linear4bit(hd, m, bias=None, compute_dtype=dt, quant_type=\"nf4\").cuda()\n",
        "                self.down_proj = Linear4bit(m, hd, bias=None, compute_dtype=dt, quant_type=\"nf4\").cuda()\n",
        "                # Force the quant_state dtype\n",
        "                self.gate_proj.weight.quant_state.dtype = dt\n",
        "                self.up_proj.weight.quant_state.dtype   = dt\n",
        "                self.down_proj.weight.quant_state.dtype = dt\n",
        "                self.act_fn = nn.SiLU()\n",
        "\n",
        "            def forward(self, x):\n",
        "                return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "\n",
        "        net = MLPTest(hd, m, dt)\n",
        "        X = torch.randn((bsz, qlen, hd), device='cuda', dtype=dt)\n",
        "\n",
        "        def do_dequant():\n",
        "            g_ = fxdequant(net.gate_proj.weight).t()\n",
        "            u_ = fxdequant(net.up_proj.weight).t()\n",
        "            d_ = fxdequant(net.down_proj.weight).t()\n",
        "            return (g_, u_, d_)\n",
        "\n",
        "        # Warm-up\n",
        "        for _ in range(2):\n",
        "            do_dequant()\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        t0 = time.time()\n",
        "        for _ in range(1000):\n",
        "            do_dequant()\n",
        "        torch.cuda.synchronize()\n",
        "        total += (time.time() - t0)\n",
        "    return total\n",
        "\n",
        "def test_final_method():\n",
        "    # Patch occupant => occupant_decode_warp_auto\n",
        "    old_fx = uutils.fast_dequantize\n",
        "    uutils.fast_dequantize = occupant_decode_warp_auto\n",
        "\n",
        "    print(\"\\n=== Testing occupant decode warp auto (with shape tuner) ===\")\n",
        "    new_time = test_dequantize_function(occupant_decode_warp_auto)\n",
        "    print(f\"[Occupant decode auto] => {new_time:.4f}s\")\n",
        "\n",
        "    # Revert\n",
        "    uutils.fast_dequantize = old_fx\n",
        "    def baseline_dequant(w):\n",
        "        return orig_fx(w, w.quant_state)\n",
        "\n",
        "    print(\"\\n=== Testing original unsloth => baseline ===\")\n",
        "    old_time = test_dequantize_function(baseline_dequant)\n",
        "    print(f\"[Unsloth Original] => {old_time:.4f}s\")\n",
        "    speedup = old_time / new_time\n",
        "    print(f\"Speedup => {speedup:.2f}x\")\n",
        "\n",
        "def test_torch_compile():\n",
        "    \"\"\"\n",
        "    Occupant decode is done outside compile => skip nib errors.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        matmul_compiled = torch.compile(lambda x, w: x @ w.t(), fullgraph=True, dynamic=True)\n",
        "    except Exception as e:\n",
        "        matmul_compiled = None\n",
        "        print(\"[WARNING] torch.compile not supported =>\", e)\n",
        "\n",
        "    if matmul_compiled is None:\n",
        "        print(\"[WARNING] skipping test_torch_compile => none\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n--- occupant decode warp auto => decode outside => compiled matmul ---\")\n",
        "    from bitsandbytes.nn import Linear4bit\n",
        "    dt = torch.float16\n",
        "    l4b = Linear4bit(128, 256, bias=None, compute_dtype=dt, quant_type=\"nf4\").cuda()\n",
        "    # Decode using occupant auto => tries multiple parameters => caches best.\n",
        "    w_dec = occupant_decode_warp_auto(l4b.weight)\n",
        "    x = torch.randn((2, 128), dtype=dt, device='cuda')\n",
        "    if x.dtype != w_dec.dtype:\n",
        "        x = x.to(w_dec.dtype)\n",
        "\n",
        "    out = matmul_compiled(x, w_dec)\n",
        "    for _ in range(10):\n",
        "        out = matmul_compiled(x, w_dec)\n",
        "    print(\"compiled_matmul => out.shape =>\", out.shape)\n",
        "\n",
        "###############################################################################\n",
        "# 6) Shape tuner load/save => do at start & end\n",
        "###############################################################################\n",
        "def main():\n",
        "    # Load tuner from disk.\n",
        "    load_shape_tuner_cache(\"shape_tuner_cache.json\")\n",
        "\n",
        "    test_final_method()\n",
        "    test_torch_compile()\n",
        "\n",
        "    # Save tuner cache to disk.\n",
        "    save_shape_tuner_cache(\"shape_tuner_cache.json\")\n",
        "\n",
        " # if __name__ == \"__main__\":\n",
        " #    main()\n",
        " #    import time\n",
        " #    time.sleep(3)\n"
      ],
      "metadata": {
        "id": "PnlARFgJJA6L"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_final_method()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ihz2zroCqY1q",
        "outputId": "911384b7-8370-464e-f528-8bcd5f031196"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Testing occupant decode warp auto (with shape tuner) ===\n",
            "[Occupant decode auto] => 5.1223s\n",
            "\n",
            "=== Testing original unsloth => baseline ===\n",
            "[Unsloth Original] => 5.5637s\n",
            "Speedup => 1.09x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_final_method()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_bzNB3CqYuT",
        "outputId": "8df850d1-5fcd-4022-9339-3301abeb62e9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Testing occupant decode warp auto (with shape tuner) ===\n",
            "[Occupant decode auto] => 5.2976s\n",
            "\n",
            "=== Testing original unsloth => baseline ===\n",
            "[Unsloth Original] => 5.3428s\n",
            "Speedup => 1.01x\n"
          ]
        }
      ]
    }
  ]
}