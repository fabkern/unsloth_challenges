{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "So2Ou1rEilxI"
      },
      "outputs": [],
      "source": [
        "# Code to install Unsloth, Triton, Torch etc\n",
        "%%capture\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helpful functions used through the entire notebook\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import set_seed\n",
        "import time\n",
        "import inspect\n",
        "import os\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "HAS_BFLOAT16 = (major_version >= 8)\n",
        "from inspect import currentframe as _C, getframeinfo\n",
        "_F = lambda c: getframeinfo(c).lineno # Gets line number\n",
        "WARN = lambda x: print(f\"\\033[31m{x}\\033[0m\") # Red colored warnings\n",
        "\n",
        "# https://stackoverflow.com/questions/18425225/getting-the-name-of-a-variable-as-a-string\n",
        "def NAME(var):\n",
        "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
        "    names = [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
        "    return names[0] if len(names) != 0 else \"\"\n",
        "\n",
        "def assert_same(x, y, line, dtype):\n",
        "    assert(x.dtype == dtype)\n",
        "    try: torch.testing.assert_close(x, y, check_stride = True)\n",
        "    except Exception as error:\n",
        "        raise RuntimeError(\n",
        "            f\"Failed allclose at line [{line}]: {NAME(x)}, {NAME(y)}\\n{str(error)}\"\n",
        "        )\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
      ],
      "metadata": {
        "id": "HMLMHGvOion5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bitsandbytes.nn import Linear4bit\n",
        "from transformers.activations import ACT2FN\n",
        "from unsloth.kernels.utils import fast_dequantize\n",
        "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
        "def unsloth_dequantize(weight):\n",
        "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
        "\n",
        "def bnb_Linear4bit(hd, m, dtype = torch.float16):\n",
        "    return Linear4bit(\n",
        "        hd, m, bias = None,\n",
        "        compute_dtype       = dtype,\n",
        "        compress_statistics = True,\n",
        "        quant_type          = \"nf4\",\n",
        "    )\n",
        "\n",
        "# [NEW] as at 18th Feb 2025\n",
        "def assert_correct_bnb(weight, dtype):\n",
        "    assert(weight.weight.dtype == torch.uint8)\n",
        "    assert(weight.weight.quant_state.dtype == dtype)\n",
        "    assert(weight.weight.quant_state.absmax.dtype == torch.uint8)\n",
        "    assert(weight.weight.quant_state.code.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.offset.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.blocksize == 64)\n",
        "    assert(weight.weight.quant_state.state2.absmax.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.state2.code.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.state2.blocksize == 256)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hd = 4096, m = 14336, dtype = torch.float16):\n",
        "        super().__init__()\n",
        "        self.gate_proj = bnb_Linear4bit(hd, m, dtype = dtype).to(\"cuda\")\n",
        "        self.up_proj   = bnb_Linear4bit(hd, m, dtype = dtype).to(\"cuda\")\n",
        "        self.down_proj = bnb_Linear4bit(m, hd, dtype = dtype).to(\"cuda\")\n",
        "        # [NEW] as at 18th Feb 2025\n",
        "        self.gate_proj.weight.quant_state.dtype = dtype\n",
        "        self.up_proj  .weight.quant_state.dtype = dtype\n",
        "        self.down_proj.weight.quant_state.dtype = dtype\n",
        "        self.act_fn = ACT2FN[\"silu\"]\n",
        "    def forward(self, x):\n",
        "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "\n",
        "def mlp_forward(X, mlp, fx):\n",
        "    up   = X @ fx(mlp.  up_proj).t()\n",
        "    gate = X @ fx(mlp.gate_proj).t()\n",
        "    h = mlp.act_fn(gate) * up\n",
        "    down = h @ fx(mlp.down_proj).t()\n",
        "    return down\n",
        "\n",
        "def mlp_dequantize(X, mlp, fx):\n",
        "    a = fx(mlp.  up_proj).t(); torch.cuda.synchronize()\n",
        "    b = fx(mlp.gate_proj).t(); torch.cuda.synchronize()\n",
        "    c = fx(mlp.down_proj).t(); torch.cuda.synchronize()\n",
        "    return a, b, c\n",
        "\n",
        "def test_dequantize(dequantize_fx):\n",
        "    elapsed = 0\n",
        "    options = [\n",
        "        (2, 3333, 2048,  8192, 3407, torch.float16),\n",
        "        (5,  777, 1024,  4096, 3409, torch.bfloat16),\n",
        "        (3, 2048, 4096, 14336, 3408, torch.bfloat16),\n",
        "    ]\n",
        "    for (bsz, qlen, hd, m, seed, dt) in options:\n",
        "        set_seed(seed)\n",
        "        torch.set_default_dtype(torch.float32)\n",
        "        mlp = MLP(hd = hd, m = m, dtype = dt)\n",
        "        X = torch.randn((bsz, qlen, hd), device = \"cuda\", dtype = dt)\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        # Warmup\n",
        "        for _ in range(2):\n",
        "            assert_same( mlp_forward(X, mlp, dequantize_fx), mlp(X), _F(_C()), dt)\n",
        "            # [NEW] as at 18th Feb 2025\n",
        "            assert_correct_bnb(mlp.  up_proj, dt)\n",
        "            assert_correct_bnb(mlp.gate_proj, dt)\n",
        "            assert_correct_bnb(mlp.down_proj, dt)\n",
        "            a, b, c = mlp_dequantize(X, mlp, dequantize_fx)\n",
        "            A, B, C = mlp_dequantize(X, mlp, unsloth_dequantize)\n",
        "            assert_same(a, A, _F(_C()), dt)\n",
        "            assert_same(b, B, _F(_C()), dt)\n",
        "            assert_same(c, C, _F(_C()), dt)\n",
        "\n",
        "        # Benchmarking\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "        for _ in range(1000): mlp_dequantize(X, mlp, dequantize_fx)\n",
        "        elapsed += time.time() - start\n",
        "    return elapsed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HK19jMDgipWI",
        "outputId": "0e47e6d6-2cf7-4d69-c940-6e076fe09fa5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "warp_persistent_dequant_3optim_best.py\n",
        "\n",
        "Implements our best-performing decode-only kernel in the conversation:\n",
        "1) Blocks-per-warp = 256 for large batch\n",
        "2) Asynchronous prefetch (tl.copy_async)\n",
        "3) Warp-level shared-memory writes for each block\n",
        "\n",
        "Typically yields ~1.05â€“1.15x speedup on decode-only in a T4 environment.\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "import unsloth.kernels.utils as uutils\n",
        "# We'll rely on puzzle's test_dequantize(unsloth_dequantize) for measurement.\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "# 1) Triton Kernel: Warp-Persistent NF4 Dequant with 3 main optimizations\n",
        "##############################################################################\n",
        "\n",
        "@triton.jit\n",
        "def _wp_dequant_nf4_kernel_3optim_best(\n",
        "    W_ptr,           # nibble-packed [n_bytes]\n",
        "    CODE_ptr,        # [16], float32 LUT\n",
        "    ABS_ptr,         # [n_blocks], block-level absmax (uint8)\n",
        "    ABS2_ptr,        # [n_blocks], second-level scale (float32)\n",
        "    OFFSET,\n",
        "    Out_ptr,\n",
        "    total_elems,\n",
        "    blocksize,\n",
        "    blocksize2,\n",
        "    out_dtype_flag,\n",
        "    BLOCKS_PER_WARP: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    Single-pass, warp-persistent decode kernel for NF4:\n",
        "      1) ~256 blocks per warp => fewer kernel calls\n",
        "      2) Asynchronous prefetch => chunk = tl.copy_async(...)\n",
        "      3) Warp-level partial writes => decode 64 nibs into shared memory, then do one coalesced store\n",
        "\n",
        "    Must have shape multiple of 64 => disclaim no partial BFS.\n",
        "    \"\"\"\n",
        "\n",
        "    pid = tl.program_id(0)\n",
        "    block_start = pid * BLOCKS_PER_WARP\n",
        "\n",
        "    # ratio => blocksize2//blocksize\n",
        "    ratio = blocksize2 // blocksize\n",
        "    n_blocks = total_elems // blocksize\n",
        "\n",
        "    # 1) Load LUT once => local array\n",
        "    code_lut = [tl.load(CODE_ptr + i) for i in range(16)]\n",
        "\n",
        "    # 2) Shared memory for warp-level partial writes => decode results\n",
        "    out_sh = tl.shared_array((64,), tl.float32)\n",
        "\n",
        "    # We'll also store combined scale in a small shared array for all blocks in this warp\n",
        "    scale_sh = tl.shared_array((256,), tl.float32)\n",
        "    idx_blk  = tl.arange(0, BLOCKS_PER_WARP)\n",
        "    global_blk_id = block_start + idx_blk\n",
        "    mask_blk = global_blk_id < n_blocks\n",
        "\n",
        "    # read abs => combine with second-level => store to scale_sh\n",
        "    abs_val_u8 = tl.load(ABS_ptr + global_blk_id, mask=mask_blk, other=0)\n",
        "    abs_val_f32 = tl.cast(abs_val_u8, tl.float32) / 127.0\n",
        "\n",
        "    nest_blk_id  = global_blk_id // ratio\n",
        "    abs2_val_f32 = tl.load(ABS2_ptr + nest_blk_id, mask=mask_blk, other=0)\n",
        "    scale_val    = abs_val_f32 * abs2_val_f32 + OFFSET\n",
        "\n",
        "    # Write each block's final scale to scale_sh\n",
        "    tl.store(scale_sh + idx_blk, scale_val, mask=mask_blk)\n",
        "    tl.barrier()\n",
        "\n",
        "    for b in range(BLOCKS_PER_WARP):\n",
        "        blk_id = block_start + b\n",
        "        if blk_id >= n_blocks:\n",
        "            break\n",
        "\n",
        "        nib_start = blk_id * blocksize\n",
        "        if nib_start >= total_elems:\n",
        "            break\n",
        "\n",
        "        # read final scale from scale_sh\n",
        "        sc = tl.load(scale_sh + b)\n",
        "\n",
        "        # asynchronous prefetch => 32 bytes => decode 64 nib\n",
        "        byte_start = nib_start // 2\n",
        "        base_addr  = byte_start + tl.arange(0, 32)\n",
        "        halfbytes  = (total_elems + 1) // 2\n",
        "        mask_load  = base_addr < halfbytes\n",
        "\n",
        "        # copy_async => overlap load with decode\n",
        "        chunk = tl.copy_async(W_ptr + base_addr, mask=mask_load, other=0)\n",
        "        tl.wait_async()\n",
        "\n",
        "        # decode nib => store in out_sh\n",
        "        for i_ in range(32):\n",
        "            b_ = chunk[i_]\n",
        "            nib_lo = b_ & 0xF\n",
        "            nib_hi = (b_ >> 4) & 0xF\n",
        "\n",
        "            code_lo = code_lut[nib_lo]\n",
        "            code_hi = code_lut[nib_hi]\n",
        "\n",
        "            out_sh[2 * i_]     = code_lo * sc\n",
        "            out_sh[2 * i_ + 1] = code_hi * sc\n",
        "\n",
        "        tl.barrier()\n",
        "\n",
        "        # coalesced store => out_sh => Out_ptr\n",
        "        i2   = tl.arange(0, 64)\n",
        "        valf = out_sh[i2]\n",
        "        if out_dtype_flag == 0:\n",
        "            out_val = tl.cast(valf, tl.float16)\n",
        "        else:\n",
        "            out_val = tl.cast(valf, tl.bfloat16)\n",
        "\n",
        "        tl.store(Out_ptr + nib_start + i2, out_val)\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "# 2) Python wrapper: best decode function\n",
        "##############################################################################\n",
        "\n",
        "def fast_dequantize_warp_persistent_3optim_best(\n",
        "    W,\n",
        "    quant_state=None,\n",
        "    out=None,\n",
        "    use_global_buffer=False,\n",
        "    BLOCKS_PER_WARP=128\n",
        "):\n",
        "    print(\"[DEBUG Py] *** fast_dequantize_warp_persistent_3optim_best ***\")\n",
        "    print(\"(1) blocks-per-warp=256, (2) async prefetch, (3) warp-level partial writes\")\n",
        "\n",
        "    if quant_state is None:\n",
        "        return W\n",
        "\n",
        "    if isinstance(quant_state, list):\n",
        "        absmax, shape, dt, bsz, comps, _, _ = quant_state\n",
        "        offset, st2 = comps\n",
        "        absmax2, code2, bsz2, _, _, _, _ = st2\n",
        "        code = None\n",
        "    else:\n",
        "        absmax  = quant_state.absmax\n",
        "        shape   = quant_state.shape\n",
        "        dt      = quant_state.dtype\n",
        "        bsz     = quant_state.blocksize\n",
        "        offset  = quant_state.offset\n",
        "        st2     = quant_state.state2\n",
        "        absmax2 = st2.absmax\n",
        "        code2   = st2.code\n",
        "        bsz2    = st2.blocksize\n",
        "        code    = quant_state.code\n",
        "\n",
        "    if code is None:\n",
        "        print(\"[DEBUG Py] code=None => returning W as-is\")\n",
        "        return W\n",
        "\n",
        "    offset_val = float(offset.item()) if isinstance(offset, torch.Tensor) else float(offset)\n",
        "    n_elems    = shape[0] * shape[1]\n",
        "    if (n_elems % bsz) != 0:\n",
        "        raise ValueError(\"decode kernel requires shape multiple of 64 => no partial BFS\")\n",
        "\n",
        "    if out is None:\n",
        "        out = torch.empty(shape, dtype=dt, device=W.device, requires_grad=False)\n",
        "    else:\n",
        "        assert out.shape == shape and out.dtype == dt\n",
        "\n",
        "    is_transposed   = (W.shape[0] == 1)\n",
        "    out_dtype_flag  = 0 if dt == torch.float16 else 1\n",
        "\n",
        "    ratio           = bsz2 // bsz\n",
        "    n_blocks        = n_elems // bsz\n",
        "    grid_dim        = math.ceil(n_blocks / BLOCKS_PER_WARP)\n",
        "\n",
        "    _wp_dequant_nf4_kernel_3optim_best[grid_dim](\n",
        "        W.data_ptr(),\n",
        "        code.data_ptr(),\n",
        "        absmax.data_ptr(),\n",
        "        absmax2.data_ptr(),\n",
        "        offset_val,\n",
        "        out.data_ptr(),\n",
        "        n_elems,\n",
        "        bsz,\n",
        "        bsz2,\n",
        "        out_dtype_flag,\n",
        "        BLOCKS_PER_WARP=BLOCKS_PER_WARP,\n",
        "        num_warps=32,\n",
        "        num_stages=7\n",
        "    )\n",
        "\n",
        "    if is_transposed:\n",
        "        return out.t()\n",
        "    return out\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "# 3) Example usage: comparing times\n",
        "##############################################################################\n",
        "\n",
        "def test_dequantize_3optim_best():\n",
        "    import unsloth.kernels.utils as uutils\n",
        "\n",
        "    # If not saved the original decode\n",
        "    if \"_original_fast_dequantize\" not in globals():\n",
        "        globals()[\"_original_fast_dequantize\"] = uutils.fast_dequantize\n",
        "\n",
        "    # Overwrite unsloth's decode\n",
        "    uutils.fast_dequantize = fast_dequantize_warp_persistent_3optim_best\n",
        "    print(\"\\n--- Testing Warp-Persistent decode approach (3optim best) ---\")\n",
        "    time_new = test_dequantize(unsloth_dequantize)\n",
        "    print(f\"[Warp-Persistent 3optim best] => {time_new:.4f}s\")\n",
        "\n",
        "    # restore\n",
        "    uutils.fast_dequantize = globals()[\"_original_fast_dequantize\"]\n",
        "    print(\"\\n--- Testing unsloth original approach ---\")\n",
        "    time_old = test_dequantize(unsloth_dequantize)\n",
        "    print(f\"[Unsloth Original] => {time_old:.4f}s\")\n",
        "\n",
        "    speedup = time_old / time_new\n",
        "    print(f\"Speedup => {speedup:.2f}x\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZPFCkrqUj3oL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dequantize_3optim_best()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7mT7qyoj8mE",
        "outputId": "80e54f65-65bb-416d-9001-51a496679847"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing Warp-Persistent decode approach (3optim best) ---\n",
            "[Warp-Persistent 3optim best] => 5.8496s\n",
            "\n",
            "--- Testing unsloth original approach ---\n",
            "[Unsloth Original] => 6.1180s\n",
            "Speedup => 1.05x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dequantize_3optim_best()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i2H-PmMvUXv",
        "outputId": "bd347ce6-9e51-4ad9-83c9-30d810970394"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing Warp-Persistent decode approach (3optim best) ---\n",
            "[Warp-Persistent 3optim best] => 5.9135s\n",
            "\n",
            "--- Testing unsloth original approach ---\n",
            "[Unsloth Original] => 6.0945s\n",
            "Speedup => 1.03x\n"
          ]
        }
      ]
    }
  ]
}
