# **QLoRA + FSDP2 + Single-Kernel Decode + `torch.compile` ‚Äì Unsloth Challenge B**

## **üìå Overview**
This repository contains my solution for **Unsloth‚Äôs Challenge B**, which involves **finetuning LLaMA 3.1 8B on 2√ó GPUs with FSDP2 and QLoRA**.  
- ‚úÖ **Fully compatible with Hugging Face‚Äôs `Trainer` and `SFTTrainer`.**  
- ‚úÖ **Uses FSDP2 to efficiently shard QLoRA LoRA-adapted weights across GPUs.**  
- ‚úÖ **Implements a Triton-based **single-kernel NF4 dequantization**, faster than naive decode.**  
- ‚úÖ **Uses `torch.compile()` to optimize LoRA modules selectively.**  
- ‚úÖ **Runs in a free Kaggle notebook with 2√ó Tesla T4 GPUs.**  

‚ö† **Pipeline parallelism with `zero bubble scheduling` is implemented but disabled by default** due to **Kaggle‚Äôs ephemeral GPU environment** potentially breaking execution.

---

## **üöÄ Features Implemented**
This script **fully implements** all required challenge components while optimizing performance.

| **Feature** | **Status** | **Implementation Proof** |
|------------|-----------|--------------------------|
| **Finetunes LLaMA 3.1 8B with FSDP2 on 2 GPUs** | ‚úÖ  | Uses `fully_shard` and `auto_wrap_policy` for `LlamaDecoderLayer`. |
| **QLoRA (4-bit NF4, frozen base, LoRA adapters added)** | ‚úÖ  | Uses `get_peft_model()` and `LoraConfig`. |
| **Mixed Precision (FP16) & CPU Offload in FSDP2** | ‚úÖ  | Configured with `MixedPrecisionPolicy` and `CPUOffloadPolicy`. |
| **Fully compatible with `Trainer` / `SFTTrainer`** | ‚úÖ  | Uses `TrainingArguments` inside `SFTTrainer`. |
| **Pipeline Parallelism (`zero bubble scheduling`)** | ‚ö† **Implemented but Disabled** | Implemented in `create_pipeline_model()`, disabled in Kaggle. |
| **Selective `torch.compile()` for LoRA submodules** | ‚úÖ  | Calls `compile_lora_modules()` to optimize LoRA layers. |
| **Minimal dataset for fast Kaggle execution** | ‚úÖ  | Uses `"train[:100]"` to **avoid ephemeral kills**. |
| **Triton-based single-kernel NF4 decode (faster than naive)** | ‚úÖ  | Implements `decode_kernel_fp16()` vs. `naive_decode_nf4()` and measures speed. |

---

## **üõ†Ô∏è Implementation Details**
### **üîπ Finetuning LLaMA 3.1 8B with QLoRA + FSDP2**
- Loads **pre-quantized NF4 4-bit LLaMA 3.1 8B** with **bitsandbytes**.
- Applies **QLoRA**, freezing all base model weights.
- Uses **FSDP2** to shard **only LoRA parameters**, distributing training across **2 GPUs**.
- Converts **frozen integer parameters to buffers** to **avoid unnecessary FSDP overhead.**
- Enables **mixed precision (`fp16`) and CPU offloading**.

### **üîπ Pipeline Parallelism with Zero Bubble Scheduling**
- **Implemented in `create_pipeline_model()` but DISABLED by default** to prevent execution failures in **Kaggle‚Äôs ephemeral environment**.
- If enabled, splits LLaMA layers across **two pipeline stages**, reducing communication overhead.
- Uses **`ScheduleInterleavedZeroBubble()`** to overlap **compute and communication**.

‚ö† **To enable pipeline parallelism, set:**
```python
os.environ["USE_PIPELINE"] = "1"
```
However, **due to ephemeral memory kills on Kaggle, this is disabled**.

### **üîπ `torch.compile()` Optimization for LoRA Modules**
- **Compiles only LoRA submodules** instead of the entire model.
- **Why?** LLaMA 3.1 8B is **too large** for `torch.compile()` to optimize in full.
- Calls **`compile_lora_modules()`** to selectively **compile only LoRA layers**, reducing compilation overhead.

### **üîπ Minimal Dataset (Avoiding Ephemeral Kills)**
- Uses **a minimal dataset slice (`train[:100]`)** to **avoid long execution times**.
- This ensures **the training process doesn‚Äôt get killed mid-run**.

### **üîπ Triton-Based Single-Kernel NF4 Decode**
- Implements **Triton single-kernel NF4 dequantization** (`decode_kernel_fp16()`).
- Compares against **naive decode** (`naive_decode_nf4()`).
- **Speedup is measured** to **prove single-kernel execution is faster**.

---

## **üñ•Ô∏è Running This in Kaggle (2√ó T4 GPUs)**
This script is **designed to run in Kaggle** with **2√ó Tesla T4 GPUs**.

### **üìå Steps to Run**
1. **Ensure your Kaggle account has access to Hugging Face models.**
2. **In Kaggle, go to "Settings" ‚Üí Enable "Accelerator = 2 GPUs".**
3. **Ensure your Hugging Face token is stored in `Kaggle Secrets`** (`HF_TOKEN`).
4. **Run this script**.

### **üíæ Expected Output**
- **Successfully loads LLaMA 3.1 8B NF4 in QLoRA mode**.
- **FSDP2 successfully shards and trains LoRA adapters**.
- **Training loss curve is equivalent to single-GPU training**.
- **Triton kernel executes single-pass NF4 decode faster than naive.**
- **Pipeline parallelism (`zero bubble scheduling`) is optionally available** but is disabled because Kaggle‚Äôs ephemeral setup may break.

## üîó **Solution Links**
- **Kaggle Notebook (QLoRA + FSDP2 on 2x T4 GPUs):** [Click Here](https://www.kaggle.com/code/rootyo/notebook-challenge-b)
