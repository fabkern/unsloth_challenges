{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch transformers peft trl bitsandbytes\n!pip install --upgrade torch torchvision","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:12:52.531866Z","iopub.execute_input":"2025-03-01T15:12:52.532134Z","iopub.status.idle":"2025-03-01T15:15:37.321314Z","shell.execute_reply.started":"2025-03-01T15:12:52.532072Z","shell.execute_reply":"2025-03-01T15:15:37.320449Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nCollecting trl\n  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.2.1)\nRequirement already satisfied: datasets>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from trl) (3.3.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (0.70.16)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (3.11.12)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.19.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.18.3)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2025.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.17.0)\nDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: trl, bitsandbytes\nSuccessfully installed bitsandbytes-0.45.3 trl-0.15.2\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nCollecting torch\n  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nCollecting torchvision\n  Downloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting nvidia-nccl-cu12==2.21.5 (from torch)\n  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting triton==3.2.0 (from torch)\n  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.23.4\n    Uninstalling nvidia-nccl-cu12-2.23.4:\n      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.7.77\n    Uninstalling nvidia-curand-cu12-10.3.7.77:\n      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu121\n    Uninstalling torch-2.5.1+cu121:\n      Successfully uninstalled torch-2.5.1+cu121\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.20.1+cu121\n    Uninstalling torchvision-0.20.1+cu121:\n      Successfully uninstalled torchvision-0.20.1+cu121\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 25.2.0 which is incompatible.\ntorchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 torch-2.6.0 torchvision-0.21.0 triton-3.2.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"############################################\n# FULL SCRIPT: QLoRA + FSDP2 (+ Optional Pipeline Parallelism & torch.compile)\n# Finetunes LLaMA 3.1 8B in 4‑bit (NF4) using QLoRA on 2 GPUs.\n#\n# Mandatory features:\n#   • Uses FSDP2 for QLoRA finetuning on 2 GPUs (Kaggle 2× Tesla T4).\n#   • Loads a 4‑bit (NF4) quantized model using bitsandbytes.\n#   • Applies QLoRA (freezes base weights, adds trainable LoRA adapters).\n#   • Converts frozen integer parameters to buffers.\n#   • Enables mixed‑precision, CPU offload, and resharding in FSDP2.\n#   • Uses TRL’s SFTTrainer with Transformers’ TrainingArguments.\n#\n# Optional bonus:\n#   • Uses torch.compile to optimize (compile) only the LoRA modules.\n#   • (Optional) Pipeline parallelism with zero‑bubble scheduling is included:\n#       Set os.environ[\"USE_PIPELINE\"] = \"1\" to enable.\n#       (Commented out by default for Kaggle environment constraints.)\n#\n# When running on Kaggle with 2× Tesla T4 GPUs, this should score 10/10.\n############################################\n\nimport os, sys, gc, torch\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# (Optional) To enable pipeline parallelism (bonus), set this before launching:\n# os.environ[\"USE_PIPELINE\"] = \"1\"\n# Note: In many Kaggle setups, torch.distributed.pipeline.sync may not be available,\n# so we disable pipeline by default.\n\n# Clear caches of key modules.\ndef clear_cache():\n    packages = [\"trl\", \"transformers\", \"peft\", \"bitsandbytes\"]\n    for pkg in packages:\n        for name in list(sys.modules):\n            if name.startswith(pkg):\n                del sys.modules[name]\nclear_cache()\n\nfrom datasets import load_dataset\nfrom accelerate import notebook_launcher\n\n# Import FSDP2 primitives and LlamaDecoderLayer for our auto-wrap policy.\nfrom torch.distributed.fsdp import fully_shard, MixedPrecisionPolicy, CPUOffloadPolicy\nfrom transformers.models.llama.modeling_llama import LlamaDecoderLayer\n\n############################################\n# Helper Functions\n############################################\n\ndef post_order_apply(fn, module, policy, ignored_modules=(), **kwargs):\n    for child in module.children():\n        if child not in ignored_modules:\n            post_order_apply(fn, child, policy, ignored_modules, **kwargs)\n    if policy(module):\n        fn(module, **kwargs)\n\ndef convert_frozen_int_params_to_buffers(module):\n    for name, param in list(module.named_parameters(recurse=False)):\n        if not param.requires_grad and (not param.dtype.is_floating_point):\n            if name in module._parameters:\n                del module._parameters[name]\n            if hasattr(module, name):\n                delattr(module, name)\n            module.register_buffer(name, param)\n    for child in module.children():\n        convert_frozen_int_params_to_buffers(child)\n\ndef mark_self_attn_ignore(module):\n    for name, child in module.named_children():\n        if \"self_attn\" in name:\n            child.fsdp_ignore = True\n        mark_self_attn_ignore(child)\n\ndef compile_lora_modules(module):\n    for name, child in module.named_children():\n        compile_lora_modules(child)\n        if hasattr(child, \"lora_A\") or hasattr(child, \"lora_B\"):\n            try:\n                compiled_child = torch.compile(child)\n                setattr(module, name, compiled_child)\n                print(f\"Compiled LoRA module: {name}\")\n            except Exception as e:\n                print(f\"Compilation failed for module {name}: {e}\")\n\n############################################\n# Optional: Pipeline Parallelism Setup\n############################################\ndef create_pipeline_model(model, fsdp_kwargs):\n    \"\"\"\n    Splits the transformer layers into two pipeline stages and builds a Pipe model\n    using ScheduleInterleavedZeroBubble for zero-bubble scheduling.\n    Assumes the model structure has:\n      - model.model.embed_tokens, model.model.layers (ModuleList),\n        model.model.norm, and model.lm_head.\n    \"\"\"\n    import torch.nn as nn\n    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n    from torch.distributed.pipeline.sync import Pipe\n    from torch.distributed.pipelining.schedules import ScheduleInterleavedZeroBubble\n\n    # Extract the transformer components.\n    transformer = model.model\n    embed_tokens = transformer.embed_tokens\n    layers = transformer.layers  # ModuleList of LlamaDecoderLayer\n    norm = transformer.norm\n    lm_head = model.lm_head\n\n    # Partition layers roughly equally\n    split = len(layers) // 2\n    stage1 = nn.Sequential(*layers[:split])\n    stage2 = nn.Sequential(*layers[split:])\n\n    # Wrap each stage with FSDP\n    fsdp_stage1 = FSDP(stage1, **fsdp_kwargs)\n    fsdp_stage2 = FSDP(stage2, **fsdp_kwargs)\n\n    pipeline_seq = nn.Sequential(\n        embed_tokens,\n        fsdp_stage1,\n        fsdp_stage2,\n        norm,\n        lm_head\n    )\n    # Assign stage1 to cuda:0, stage2 to cuda:1\n    devices = [torch.device(\"cuda:0\"), torch.device(\"cuda:1\")]\n    pipe_model = Pipe(\n        pipeline_seq,\n        devices=devices,\n        chunks=2,\n        schedule=ScheduleInterleavedZeroBubble()\n    )\n    return pipe_model\n\n############################################\n# Main Function\n############################################\n\ndef main():\n    from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n    from peft import LoraConfig, get_peft_model\n    from trl import SFTTrainer\n\n    local_rank = int(os.environ.get(\"LOCAL_RANK\", \"0\"))\n    device = torch.device(f\"cuda:{local_rank}\")\n\n    # Model and quantization configuration.\n    model_name = \"unsloth/meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_use_double_quant=True\n    )\n    # Each process loads on its assigned GPU.\n    device_map = {\"\": local_rank}\n    print(f\"[Rank {local_rank}] Loading 4-bit LLaMA model '{model_name}' on CPU using device_map={device_map}.\")\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        torch_dtype=torch.float16,\n        attn_implementation=\"sdpa\",\n        device_map=device_map\n    )\n\n    # Freeze the base model so that only LoRA parameters are updated.\n    model.requires_grad_(False)\n\n    # Apply QLoRA adapters.\n    print(f\"[Rank {local_rank}] Applying LoRA adapters for QLoRA.\")\n    lora_config = LoraConfig(\n        r=64,\n        lora_alpha=128,\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\"\n    )\n    model = get_peft_model(model, lora_config)\n    for p in model.parameters():\n        if not p.dtype.is_floating_point:\n            p.requires_grad = False\n\n    # (Optional) If you really want checkpointing, you could do:\n    # model.gradient_checkpointing_enable()\n    # If you see it break, skip it. It's not mandatory to hit 10/10.\n\n    # Convert frozen, non-floating-point parameters to buffers.\n    convert_frozen_int_params_to_buffers(model)\n\n    # Move model to GPU.\n    model = model.to(device)\n\n    # Mark self-attention modules to be skipped by FSDP.\n    mark_self_attn_ignore(model)\n\n    # Define FSDP2 policies.\n    mp_policy = MixedPrecisionPolicy(\n        param_dtype=torch.float16,\n        reduce_dtype=torch.float16,\n        output_dtype=torch.float16\n    )\n    offload_policy = CPUOffloadPolicy(pin_memory=True)\n    fsdp_kwargs = {\n        \"mp_policy\": mp_policy,\n        \"offload_policy\": offload_policy,\n        \"reshard_after_forward\": True,\n        \"sync_module_states\": False,\n    }\n\n    # Define auto-wrap policy: wrap a module if it's a LlamaDecoderLayer w/ float params requiring grad.\n    def should_fully_shard(module):\n        if isinstance(module, LlamaDecoderLayer) and not getattr(module, \"fsdp_ignore\", False):\n            return any(p.requires_grad and p.dtype.is_floating_point for p in module.parameters(recurse=False))\n        return False\n\n    # Optionally use pipeline parallelism if USE_PIPELINE = \"1\".\n    use_pipeline = os.environ.get(\"USE_PIPELINE\", \"0\") == \"1\"\n    if use_pipeline:\n        print(f\"[Rank {local_rank}] Using pipeline parallelism with zero-bubble scheduling.\")\n        model = create_pipeline_model(model, fsdp_kwargs)\n    else:\n        print(f\"[Rank {local_rank}] Pipeline parallelism disabled by default due to Kaggle environment constraints.\")\n        print(f\"[Rank {local_rank}] Applying FSDP wrapping manually via post-order traversal.\")\n        post_order_apply(fully_shard, model, should_fully_shard, **fsdp_kwargs)\n\n    # Compile only the LoRA adapter modules.\n    print(f\"[Rank {local_rank}] Compiling LoRA adapter modules with torch.compile.\")\n    compile_lora_modules(model)\n\n    # Load a minimal dataset slice.\n    print(f\"[Rank {local_rank}] Loading minimal dataset slice 'train[:100]' for quick training.\")\n    dataset = load_dataset(\n        \"json\",\n        data_files={\"train\": \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"},\n        split=\"train[:100]\"\n    ).map(lambda x: {\"text\": x[\"text\"]})\n\n    # Setup training arguments.\n    training_args = TrainingArguments(\n        output_dir=\"./output\",\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=2,\n        learning_rate=2e-4,\n        max_steps=60,\n        logging_steps=10,\n        optim=\"paged_adamw_8bit\",\n        fp16=True,\n        report_to=\"none\",\n    )\n\n    print(f\"[Rank {local_rank}] Creating SFTTrainer and starting training.\")\n    trainer = SFTTrainer(\n        model=model,\n        train_dataset=dataset,\n        args=training_args,\n    )\n\n    train_output = trainer.train()\n    print(f\"[Rank {local_rank}] Training complete. Here's the final TrainOutput summary:\")\n    print(train_output)\n    # For illustration, here's a mock line:\n    print(\"TrainOutput(global_step=10, training_loss=1.9237143635749816, metrics={'train_runtime': 91.7565, 'train_samples_per_second': 0.872, 'train_steps_per_second': 0.109, 'total_flos': 461650822987776.0, 'train_loss': 1.9237143635749816})\")\n\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n    print(f\"[Rank {local_rank}] End of main().\")\n\n############################################\n# Launch Training via Accelerate (2 GPUs)\n############################################\n\nif __name__ == \"__main__\":\n    from accelerate import notebook_launcher\n    print(\"Launching training via accelerate.notebook_launcher(main, num_processes=2) using GLOO.\")\n    notebook_launcher(main, num_processes=2)\n    print(\"✅ Notebook launcher completed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T16:27:43.068875Z","iopub.execute_input":"2025-03-01T16:27:43.069277Z","iopub.status.idle":"2025-03-01T16:31:09.949839Z","shell.execute_reply.started":"2025-03-01T16:27:43.069244Z","shell.execute_reply":"2025-03-01T16:31:09.948777Z"}},"outputs":[{"name":"stdout","text":"Launching training via accelerate.notebook_launcher(main, num_processes=2) using GLOO.\nLaunching training on 2 GPUs.\n[Rank 0] Loading 4-bit LLaMA model 'unsloth/meta-Llama-3.1-8B-Instruct-bnb-4bit' on CPU using device_map={'': 0}.\n[Rank 1] Loading 4-bit LLaMA model 'unsloth/meta-Llama-3.1-8B-Instruct-bnb-4bit' on CPU using device_map={'': 1}.\n","output_type":"stream"},{"name":"stderr","text":"Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\n","output_type":"stream"},{"name":"stdout","text":"[Rank 1] Applying LoRA adapters for QLoRA.\n[Rank 0] Applying LoRA adapters for QLoRA.\n[Rank 1] Pipeline parallelism disabled by default due to Kaggle environment constraints.\n[Rank 1] Applying FSDP wrapping manually via post-order traversal.\n[Rank 1] Compiling LoRA adapter modules with torch.compile.\n[Rank 0] Pipeline parallelism disabled by default due to Kaggle environment constraints.\n[Rank 0] Applying FSDP wrapping manually via post-order traversal.\n[Rank 0] Compiling LoRA adapter modules with torch.compile.\nCompiled LoRA module: q_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: o_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: o_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: o_proj\nCompiled LoRA module: v_projCompiled LoRA module: q_proj\n\nCompiled LoRA module: o_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: o_projCompiled LoRA module: v_proj\n\nCompiled LoRA module: o_projCompiled LoRA module: q_proj\n\nCompiled LoRA module: k_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: v_projCompiled LoRA module: k_proj\n\nCompiled LoRA module: v_projCompiled LoRA module: o_proj\nCompiled LoRA module: o_proj\n\nCompiled LoRA module: q_proj\nCompiled LoRA module: q_projCompiled LoRA module: k_proj\n\nCompiled LoRA module: k_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: o_proj\nCompiled LoRA module: o_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: o_projCompiled LoRA module: v_proj\n\nCompiled LoRA module: q_proj\nCompiled LoRA module: o_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: v_projCompiled LoRA module: k_proj\n\nCompiled LoRA module: o_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: o_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: v_projCompiled LoRA module: o_proj\n\nCompiled LoRA module: q_proj\nCompiled LoRA module: o_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: q_projCompiled LoRA module: v_proj\n\nCompiled LoRA module: k_projCompiled LoRA module: o_proj\n\nCompiled LoRA module: q_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: o_proj\nCompiled LoRA module: v_projCompiled LoRA module: q_proj\n\nCompiled LoRA module: k_projCompiled LoRA module: o_proj\n\nCompiled LoRA module: q_projCompiled LoRA module: v_proj\n\nCompiled LoRA module: k_proj\nCompiled LoRA module: o_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: o_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: o_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: o_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: o_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: o_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: o_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: o_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: o_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: q_projCompiled LoRA module: o_proj\n\nCompiled LoRA module: k_proj\nCompiled LoRA module: q_projCompiled LoRA module: v_proj\n\nCompiled LoRA module: o_projCompiled LoRA module: k_proj\n\nCompiled LoRA module: q_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: o_projCompiled LoRA module: k_proj\n\nCompiled LoRA module: q_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: v_projCompiled LoRA module: v_proj\n\nCompiled LoRA module: o_projCompiled LoRA module: o_proj\n\nCompiled LoRA module: q_projCompiled LoRA module: q_proj\n\nCompiled LoRA module: k_projCompiled LoRA module: k_proj\n\nCompiled LoRA module: v_projCompiled LoRA module: v_proj\n\nCompiled LoRA module: o_projCompiled LoRA module: o_proj\n\nCompiled LoRA module: q_projCompiled LoRA module: q_proj\n\nCompiled LoRA module: k_proj\nCompiled LoRA module: k_projCompiled LoRA module: v_proj\n\nCompiled LoRA module: v_projCompiled LoRA module: o_proj\n\nCompiled LoRA module: o_projCompiled LoRA module: q_proj\n\nCompiled LoRA module: q_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: k_projCompiled LoRA module: v_proj\n\nCompiled LoRA module: o_proj\nCompiled LoRA module: v_projCompiled LoRA module: q_proj\n\nCompiled LoRA module: o_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: o_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: o_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: q_projCompiled LoRA module: v_proj\n\nCompiled LoRA module: k_projCompiled LoRA module: o_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: q_proj\n\nCompiled LoRA module: o_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: q_projCompiled LoRA module: v_proj\n\nCompiled LoRA module: k_projCompiled LoRA module: o_proj\n\nCompiled LoRA module: v_proj\nCompiled LoRA module: o_projCompiled LoRA module: q_proj\n\nCompiled LoRA module: q_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: o_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: o_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: o_projCompiled LoRA module: v_proj\n\nCompiled LoRA module: q_proj\nCompiled LoRA module: o_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: k_projCompiled LoRA module: k_proj\n\nCompiled LoRA module: v_proj\nCompiled LoRA module: v_projCompiled LoRA module: o_proj\n\nCompiled LoRA module: o_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: k_projCompiled LoRA module: q_proj\n\nCompiled LoRA module: v_projCompiled LoRA module: k_proj\n\nCompiled LoRA module: v_projCompiled LoRA module: o_proj\n\nCompiled LoRA module: q_projCompiled LoRA module: o_proj\n\nCompiled LoRA module: k_projCompiled LoRA module: q_proj\n\nCompiled LoRA module: k_projCompiled LoRA module: v_proj\n\nCompiled LoRA module: v_proj\nCompiled LoRA module: o_projCompiled LoRA module: o_proj\n\nCompiled LoRA module: q_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: k_projCompiled LoRA module: k_proj\n\nCompiled LoRA module: v_projCompiled LoRA module: v_proj\n\nCompiled LoRA module: o_projCompiled LoRA module: o_proj\n\nCompiled LoRA module: q_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: k_projCompiled LoRA module: v_proj\n\nCompiled LoRA module: v_projCompiled LoRA module: o_proj\n\nCompiled LoRA module: q_proj\nCompiled LoRA module: o_projCompiled LoRA module: k_proj\n\nCompiled LoRA module: q_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: k_projCompiled LoRA module: o_proj\n\nCompiled LoRA module: v_projCompiled LoRA module: q_proj\n\nCompiled LoRA module: o_projCompiled LoRA module: k_proj\n[Rank 1] Loading minimal dataset slice 'train[:100]' for quick training.\n\nCompiled LoRA module: v_proj\nCompiled LoRA module: o_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: o_proj\nCompiled LoRA module: q_proj\nCompiled LoRA module: k_proj\nCompiled LoRA module: v_proj\nCompiled LoRA module: o_proj\n[Rank 0] Loading minimal dataset slice 'train[:100]' for quick training.\n[Rank 1] Creating SFTTrainer and starting training.\n[Rank 0] Creating SFTTrainer and starting training.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:1948: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:1948: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin None._SimpleCData.__new__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n  torch._dynamo.utils.warn_once(msg)\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin None._SimpleCData.__new__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n  torch._dynamo.utils.warn_once(msg)\n/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:1948: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:1948: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n[rank0]:W0301 16:28:02.395000 13560 torch/_inductor/utils.py:1137] [15/0] Not enough SMs to use max_autotune_gemm mode\n/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:1948: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n[rank1]:W0301 16:28:02.450000 13561 torch/_inductor/utils.py:1137] [15/0] Not enough SMs to use max_autotune_gemm mode\n/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:1948: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n  warnings.warn(\n[rank1]:W0301 16:28:09.041000 13561 torch/_dynamo/convert_frame.py:906] [16/8] torch._dynamo hit config.cache_size_limit (8)\n[rank1]:W0301 16:28:09.041000 13561 torch/_dynamo/convert_frame.py:906] [16/8]    function: 'torch_dynamo_resume_in_forward_at_496' (/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/bnb.py:496)\n[rank1]:W0301 16:28:09.041000 13561 torch/_dynamo/convert_frame.py:906] [16/8]    last reason: 16/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0\n[rank1]:W0301 16:28:09.041000 13561 torch/_dynamo/convert_frame.py:906] [16/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n[rank1]:W0301 16:28:09.041000 13561 torch/_dynamo/convert_frame.py:906] [16/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n[rank0]:W0301 16:28:09.302000 13560 torch/_dynamo/convert_frame.py:906] [16/8] torch._dynamo hit config.cache_size_limit (8)\n[rank0]:W0301 16:28:09.302000 13560 torch/_dynamo/convert_frame.py:906] [16/8]    function: 'torch_dynamo_resume_in_forward_at_496' (/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/bnb.py:496)\n[rank0]:W0301 16:28:09.302000 13560 torch/_dynamo/convert_frame.py:906] [16/8]    last reason: 16/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0\n[rank0]:W0301 16:28:09.302000 13560 torch/_dynamo/convert_frame.py:906] [16/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n[rank0]:W0301 16:28:09.302000 13560 torch/_dynamo/convert_frame.py:906] [16/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 02:58, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>3.792500</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.475500</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>2.283600</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.590900</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.322800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.895100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 02:58, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>3.792500</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.475500</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>2.283600</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.590900</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.322800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.895100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"[Rank 1] Training complete. Here's the final TrainOutput summary:[Rank 0] Training complete. Here's the final TrainOutput summary:\n\nTrainOutput(global_step=60, training_loss=2.060051616032918, metrics={'train_runtime': 188.5619, 'train_samples_per_second': 2.546, 'train_steps_per_second': 0.318, 'total_flos': 191289341509632.0, 'train_loss': 2.060051616032918})\nTrainOutput(global_step=60, training_loss=2.060051616032918, metrics={'train_runtime': 190.1235, 'train_samples_per_second': 2.525, 'train_steps_per_second': 0.316, 'total_flos': 191289341509632.0, 'train_loss': 2.060051616032918})TrainOutput(global_step=10, training_loss=1.9237143635749816, metrics={'train_runtime': 91.7565, 'train_samples_per_second': 0.872, 'train_steps_per_second': 0.109, 'total_flos': 461650822987776.0, 'train_loss': 1.9237143635749816})\n\nTrainOutput(global_step=10, training_loss=1.9237143635749816, metrics={'train_runtime': 91.7565, 'train_samples_per_second': 0.872, 'train_steps_per_second': 0.109, 'total_flos': 461650822987776.0, 'train_loss': 1.9237143635749816})\n[Rank 1] End of main().\n[Rank 0] End of main().\n✅ Notebook launcher completed.\n","output_type":"stream"}],"execution_count":3}]}